{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech Signal Analysis\n",
    "======================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech is produced by the excitation of the time-varying vocal tract system by a\n",
    "time-varying source (vibrations of vocal cords). The excitation is generated by\n",
    "air flow from the lungs carried by the trachea through the vocal cords. As the\n",
    "acoustic wave passes through the vocal tract, its frequency content (spectrum)\n",
    "is altered by the resonances of the vocal tract. Vocal tract resonances are\n",
    "called formants. Thus, the vocal tract shape can be estimated from the spectral\n",
    "shape (e.g. formant location and spectral tilt) of the speech signal. The speech\n",
    "produced is an acoustic wave that is recorded, sampled, quantized and stored on\n",
    "the computer as a sequence of numbers (signal). The speech signal can't be used\n",
    "directly, as the information is in the sequence of the numbers. So the speech\n",
    "signal has to be processed and then features relevant to the task have to be\n",
    "extracted. The extracted features may be related to the voice source, i.e. vocal\n",
    "cords, like pitch frequency, pitch frequency contour etc. or the vocal tract\n",
    "system, like linear prediction parameters, cepstral etc. In this laboratory\n",
    "session, we are going to learn about speech signal processing and extraction of\n",
    "features related to voice source and vocal-tract system.\n",
    "\n",
    "We will conduct the following experiments:\n",
    "\n",
    "1.  In a 2 second speech signal, you will observe that there is more energy in\n",
    "    speech regions than in non-speech regions.\n",
    "2.  The speech signal is non-stationary in nature, so it is processed as a\n",
    "    short-time signal where it is quasi-stationary. In the second experiment, we\n",
    "    will select a short-time speech signal and estimate the pitch frequency\n",
    "    manually.\n",
    "3.  We will observe the autocorrelation of the short-time signal and compute the\n",
    "    pitch frequency from it.\n",
    "4.  We will estimate the Fourier spectrum of the short-time signal and also study\n",
    "    the effect of windowing.\n",
    "5.  We will study the spectrogram of the speech signal observed in the first experiment.\n",
    "6.  We will learn about linear prediction analysis and study the vocal-tract response,\n",
    "    like formants and voice source feature like pitch frequency.\n",
    "7.  We will perform linear prediction analysis on different speech\n",
    "    sound signals and observe that they have distinct features.\n",
    "8.  Although the features are distinctive in nature they can vary, which makes tasks such as\n",
    "    speech recognition or speaker recognition difficult. Thus we will study\n",
    "    variability introduced by speakers.\n",
    "9.  Finally, we will study the pitch contour and the information embedded in it.\n",
    "\n",
    "The sampling frequency *sf* of all speech signals is 16000 Hz. Every file name\n",
    "contains information about the gender, speaker, trial number and the sound, for\n",
    "example the speech signal file `f_s1_t1_a` means it is the vowel /​a/ spoken\n",
    "by female (`f`), (for male its `m` and child `c`) speaker 1 (`s1`) and trial\n",
    "number 1 (`t1`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = \"data/f_s1_t1_a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have problems in using any of the functions, use `help` to\n",
    "know the usage, for example\n",
    "\n",
    "    help(speech_signal_observation)\n",
    "\n",
    "will give the usage of the function `speech_signal_observation()`.\n",
    "\n",
    "Note: The speech files are stored in ascii format so kindly don't edit\n",
    "or tamper with them. In all the experiments, the usage of the function is\n",
    "explained and then an example usage is given. Follow the example usage\n",
    "for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Signal Observation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 2 second speech utterance using the\n",
    "`speech_signal_observation()` function and observe the envelope of the\n",
    "signal. The usage of the function is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech_analysis import *\n",
    "help(speech_signal_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots the speech signal specified with `filename` with an optional title\n",
    "and returns the speech signal array, for example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = speech_signal_observation(utterance, title=utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the speech utterance plotted in the upper part of the figure\n",
    "and the short-time energy plotted below, which is the envelope of the speech\n",
    "signal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation of the Short-Time Speech Signal and Manual Pitch Computation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speech signal is non-stationary in nature but it can be assumed to be\n",
    "quasi-stationary for one to three pitch periods (short-time signal). In this\n",
    "experiment, we are going to observe a short-time speech signal. Select a 30 ms\n",
    "window from the 2 second speech signal observed in experiment by\n",
    "using the `select_speech()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(select_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data = select_speech(data, 15000, 15480,\n",
    "                       \"30 ms window of utterance \" + utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the damped sinusoids repeated periodically. Find the period of\n",
    "each sinusoid (neglect the sinusoids which are not complete in the\n",
    "plot) in the following way:\n",
    "\n",
    "1.  Note down the sample number of the largest peak of each sinusoid.\n",
    "2.  Find the number of samples between each of the consecutive peaks. It gives\n",
    "    the period of each sinusoid.\n",
    "\n",
    "Average these periods to estimate the pitch period, $p_{t}$. Calculate the\n",
    "fundamental frequency or pitch frequency $F_{0}$ using the following equation\n",
    "($sf$ *is the sampling frequency*)\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{org2d6dd92}\n",
    "  F_{0} = \\frac{sf}{p_{t}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we compute the autocorrelation of the short-time\n",
    "speech signal obtained from the Experiment using the\n",
    "`autocorrelation()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(autocorrelation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = autocorrelation(\n",
    "    st_data, 256,\n",
    "    \"256-lag autocorrelation of the 30 ms window of utterance \" + utterance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the autocorrelation array is *lag + lag + 1* which is\n",
    "symmetric to the point *lag + 1* (for the above example it is 257). The\n",
    "value at this point is the energy of the short-time signal for which the\n",
    "autocorrelation was computed. The upper plot shows the actual autocorrelation\n",
    "(observe the symmetricity) and the plot below shows the right-half symmetry\n",
    "(i.e. from *lag + 1* to *lag + lag + 1*). Find the second peak in this plot and note\n",
    "down the lag number, it is the pitch period $p_{t}$. Use equation to\n",
    "find the fundamental frequency $F_{0}$. Compare it with the $F_{0}$ obtained in\n",
    "the previous experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Spectrum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we compute the Fourier spectrum of the short-time\n",
    "signal `st_data` obtained in Experiment using the\n",
    "`fourier_spectrum()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(fourier_spectrum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It computes the DFT of order `order` of the short-time signal `data`. The order\n",
    "of the DFT is generally chosen such that it is a $2^{n}$ value to take advantage\n",
    "of the FFT routine. Depending upon the number of samples, select the order of\n",
    "FFT which is near to it, for example the 30 ms window we are using has 480\n",
    "samples, so we select an order of 512:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_spectrum(\n",
    "    st_data, 512,\n",
    "    title=\"Fourier spectrum of the 30 ms window of utterance \" + utterance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper plot shows the 512-point DFT spectrum (observe the symmetricity) and\n",
    "the plot below shows the left symmetry of the plot (from point 1 to 256).\n",
    "Observe the spectral peaks, which are the formants (resonances in the vocal\n",
    "tract). The 512-point range covers the entire sampling frequency range, i.e.\n",
    "16000 Hz, which has redundant information, whereas the plot below covers half of\n",
    "the sampling frequency, i.e. 8000Hz, which is the region of interest (recall the\n",
    "sampling theorem).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowed Speech Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window the short-time speech signal `st_data` with the Hanning window:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "hanning_window = np.hanning(len(st_data))\n",
    "plt.plot(hanning_window)\n",
    "st_data_hanning = st_data * hanning_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Fourier spectrum for the windowed short-time signal\n",
    "`st_data_hanning`. Observe the difference in the Fourier spectrum of the signal\n",
    "`st_data` using a rectangular window (which was implicit when we created it in\n",
    "Experiment ) and the signal `st_data_hanning` using the Hanning\n",
    "window.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_spectrum(\n",
    "    st_data_hanning, 512,\n",
    "    title=\"Fourier spectrum of the 30 ms Hanning window of utterance \" + utterance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we are going to compute the narrow-band and\n",
    "wide-band spectrogram of the entire utterance i.e. the signal `data`\n",
    "obtained in Experiment . Recall that in the\n",
    "wide-band spectrogram we get good time\n",
    "resolution and in the narrow-band spectrogram we get good frequency resolution. The\n",
    "spectrogam is computed using the `spectrogram()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of spectrogram depends upon the order `order`. For a wide-band\n",
    "spectrogram we need a small window and choose order 256 or 128, which is a\n",
    "short duration. For a narrow-band spectrogram we choose order 1024 or\n",
    "2048, which is a long duration, so we loose the time resolution. The function\n",
    "uses the Hanning window internally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide-band Spectrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram(data, 256, title=\"Wide-band spectrogram of utterance \" + utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow-band Spectrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram(data, 1024, title=\"Narrow-band spectrogram of utterance \" + utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Prediction (LP) Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear prediction is the most common technique to estimate the\n",
    "shape of the vocal tract. A $p\\text{-th}$ order linear prediction expresses\n",
    "every sample as the linear weighted sum of the past $p$ samples. The\n",
    "resulting difference equation expressed in the $z\\text{-domain}$ is\n",
    "\n",
    "$$\n",
    "H(z) = \\frac{1}{1 - \\sum_{j = 1}^{p} a_{j}z^{-j}}\n",
    "$$\n",
    "\n",
    "The idea behind\n",
    "linear prediction analysis is to estimate the $p$ $a_{k}\\text{-s}$\n",
    "that minimize the mean-square error of the prediction. The linear\n",
    "prediction error is also called LP residual. The $a_{k}\\text{-s}$\n",
    "determine the solution of the equation. The solution of the equation\n",
    "in the denominator is called pole. A real pole determines the spectral\n",
    "roll-off and a complex pole (which always exists with a conjugate)\n",
    "determines the location of the formant in the LP spectrum. The LP\n",
    "spectrum is the Fourier transform of the $a_{k}\\text{-s}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LP Spectrum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will observe the LP spectrum of the short-time\n",
    "speech signal obtained from Experiment using\n",
    "the function `lp_spectrum()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lp_spectrum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lp_order` is the linear prediction order $p$. The default `window` function is\n",
    "the Hanning window. `order` is the FFT order needed to compute the linear\n",
    "prediction spectrum from the $a_{k}\\text{-s}$. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_order = 14\n",
    "_ = lp_spectrum(st_data, lp_order, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure, you will observe two plots. The upper plot is\n",
    "the Fourier spectrum and the lower plot is the linear prediction spectrum.\n",
    "Observe the more prominent spectral peaks (formants) in the linear prediction spectrum compared\n",
    "to the Fourier spectrum. Note down the frequency of each peak, then go back to\n",
    "the wide-band spectrogram from Experiment and observe\n",
    "that the energy is indeed high near that spectral frequency. Now change the linear prediction\n",
    "order `lp_order` to, say 1, 3, 16, 20, 30, 50 and observe the changes in\n",
    "the LP spectrum. Try to reason about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LP Residual\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will perform linear\n",
    "prediction analysis and compute the LP residual of the short-time speech signal\n",
    "obtained from Experiment with the `lp_residual()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lp_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_order = 10\n",
    "residual = lp_residual(st_data, lp_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Note down the sample number of the largest peak of each sinusoid in\n",
    "    the upper plot.\n",
    "2.  Note down the sample number of the corresponding peaks in the LP residual.\n",
    "3.  Compare these two observations. Are they the same?\n",
    "\n",
    "Perform an autocorrelation analysis on the residual signal using\n",
    "the `autocorrelation()` function and find the pitch period as it was done\n",
    "in Experiment :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = autocorrelation(residual, 256, title=\"Autocorrelation of the LP residual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP Spectrum of Different Speech Sounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Experiment , we studied the LP spectrum of a short-time signal. In\n",
    "this experiment, we are going to study the LP spectra of different vowels. Note\n",
    "down your observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /​a/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data_a = load_signal(\"data/m_s2_t1_a\")[9000:9480]\n",
    "_ = lp_spectrum(st_data_a, 16, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /​e/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data_e = load_signal(\"data/m_s2_t1_e\")[8000:8480]\n",
    "_ = lp_spectrum(st_data_e, 16, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /​i/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data_i = load_signal(\"data/m_s2_t1_i\")[14000:14480]\n",
    "_ = lp_spectrum(st_data_i, 14, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /​o/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data_o = load_signal(\"data/m_s2_t1_o\")[12000:12480]\n",
    "_ = lp_spectrum(st_data_o, 18, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /​u/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data_u = load_signal(\"data/m_s2_t1_u\")[17000:17480]\n",
    "_ = lp_spectrum(st_data_u, 18, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra- and Inter-Speaker Variability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Experiments and , we studied the effect of order on linear\n",
    "  prediction and also observed that for different sounds the formants are\n",
    "  different. In this experiment, we are going to analyse the variability caused\n",
    "  by speakers. There are two kinds of speaker variability that are of interest:\n",
    "\n",
    "1.  **Intra-speaker variability** is the\n",
    "    variability introduced by the same speaker while producing the same sound\n",
    "    repeatedly.\n",
    "2.  **Inter-speaker variability** is the variability introduced by\n",
    "    different speakers producing the same sound.\n",
    "\n",
    "This can be useful depending upon the type of application, such as in speech\n",
    "recognition it is good if there is no speaker variability, whereas, for speaker\n",
    "recognition inter-speaker variability is very important. Intra-speaker\n",
    "variability is neither useful for speech recognition nor for speaker recognition\n",
    "applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Speaker Variability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we use 3 utterances of the same sound /​a/ spoken by the\n",
    "same speaker 3 different times. We will use the `speaker_variation()` function,\n",
    "which takes the utterance file names and their corresponding starting points\n",
    "defining the short-time signal. A length of 480 samples for the short-time\n",
    "signal is assumed by default.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(speaker_variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_variation([(\"data/f_s2_t1_a\", 14000),\n",
    "                   (\"data/f_s2_t2_a\", 10000),\n",
    "                   (\"data/f_s2_t3_a\", 12480)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computes the LP spectrum of the short-time signal of all 3 utterances and\n",
    "plots them in the same figure. Observe that the first two formant regions for all\n",
    "3 utterances are almost the same, while this is not the case for higher formants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Speaker Variability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take 3 utterances of the sound /​a/ spoken by a female, male and a child.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_variation([(\"data/f_s1_t1_a\", 15000),\n",
    "                   (\"data/m_s2_t1_a\", 9000),\n",
    "                   (\"data/c_s1_t1_a\", 12480)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again observe that the first two formant regions for the male and female speaker\n",
    "are almost the same. In case of child speech the second formant shifted much\n",
    "more than the first formant. Like in the previous experiment we observe that the\n",
    "higher formant regions are different for different speakers even though the same\n",
    "sound /​a/ is being spoken.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT Algorithm and Pitch Contour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we extend the idea of pitch estimation using the LP residual\n",
    "(see Experiment ) into a pitch estimation algorithm. The pitch frequency\n",
    "can be estimated through the Simple Inverse Filter Tracking (SIFT) algorithm. It\n",
    "computes the pitch frequency for a given short-time speech signal in the following\n",
    "way:\n",
    "\n",
    "1.  Low-pass-filter the short-time signal.\n",
    "2.  Perform LP analysis and obtain the LP residual.\n",
    "3.  Perform autocorrelation on the LP residual.\n",
    "4.  Find the location of the second peak, make a decision on voicing. If voiced, compute\n",
    "    the pitch frequency, else set the pitch frequency to zero.\n",
    "\n",
    "The pitch frequency contour for a spoken sentence can be computed by taking a\n",
    "short-time window of, for example, size 30 ms:\n",
    "\n",
    "1.  Place this window at the beginning of the speech signal and compute the pitch\n",
    "    frequency using the SIFT algorithm.\n",
    "2.  Shift the window by 10 ms and compute the pitch frequency using the SIFT algorithm.\n",
    "3.  Repeat step 2 until the end of the speech signal is reached.\n",
    "\n",
    "The 10 ms shift is called a **frame**. So we obtain a pitch frequency for every 10\n",
    "ms or every frame. The pitch contour is nothing but the array of pitch\n",
    "frequencies obtained for the sequence of frames. For applications like speech or\n",
    "speaker recognition, for every frame a feature parameter vector (e.g. LP\n",
    "coefficients) is obtained. In other words, the feature extraction stage yields a\n",
    "sequence of feature parameter vectors $x_1, x_2 \\cdots x_{N-1}, x_N$, where $N$\n",
    "is the number of frames.\n",
    "\n",
    "In this experiment, first, we are going to observe the pitch contour\n",
    "for two different types of sentences, interrogative and declarative,\n",
    "using the `sift()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this study, we will use a 30 ms frame size (480 samples), a shift of 10 ms\n",
    "(160 samples) and a linear prediction order of 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_order = 10\n",
    "frame_size = 480\n",
    "frame_shift = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence spoken is an interrogative sentence, *\"Where are you from?\"*:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sift(\"data/m_s1_i_sen1\", lp_order, frame_size, frame_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot you see the speech signal and its pitch contour below. Observe the\n",
    "rise and fall of the pitch contour across the sentence. This rise and fall of\n",
    "pitch contour carries information like speaking style, type of sentence,\n",
    "emotional status of the speaker etc. Observe the rise of the pitch contour for\n",
    "the word *where* at the beginning of the sentence (in the context of\n",
    "interrogation). If a line is drawn interpolating the peaks and valleys in the\n",
    "pitch contour, it will have a positive slope. Observe at the end again a fall\n",
    "and then a rise of the pitch contour.\n",
    "\n",
    "The next sentence is a declarative sentence, *\"I am from India\"*:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sift(\"data/m_s1_d_sen1\", lp_order, frame_size, frame_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the rise and fall of pitch contour across the sentence. If a line is\n",
    "drawn interpolating the peaks and valleys in this pitch contour, it will have a\n",
    "negative slope.\n",
    "\n",
    "Note that the pitch frequency for a single frame is just an information about\n",
    "the speaker. It doesn't convey any information regarding the sentence being\n",
    "spoken or its message or the emotional status of speaker. But when a longer\n",
    "duration (say 100-300 ms), i.e. a sequence of frames, is considered then we can\n",
    "observe the rise and fall of the pitch contour and derive such information.\n",
    "Still, the pitch contour does not convey any information regarding the message\n",
    "being spoken.\n",
    "\n",
    "Now we will perform the pitch contour analysis on the same sentences spoken by a\n",
    "different speaker:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sift(\"data/m_s2_i_sen1\", lp_order, frame_size, frame_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sift(\"data/m_s2_d_sen1\", lp_order, frame_size, frame_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these pitch contours to those of the previous speaker who spoke the\n",
    "same sentences. Are they different?  Human efficiently use the\n",
    "speaking style information which is embedded in the pitch contour to\n",
    "recognize another person.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab was originally developed in Matlab for the *Speech Processing and\n",
    "Speech Recognition* course at École polytechnique fédérale de Lausanne (EPFL).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
